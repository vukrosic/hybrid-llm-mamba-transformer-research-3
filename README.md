THIS REPO IS NOT USED IN ANY VIDEO OR FINISHED, just some random experiments I did, readme is not 100% accurate, you can use coding AI IDE and let it help you with this repo

# 🚀 Hybrid Mamba-Transformer LLM Research

I recommend you use AI coding IDE:
- Gemini CLI is free, you can connect it to VS code
- Cline + Cerebras is also free

Ask it to help you navigate this repo. I made it for the video, but README is generated by AI.

YouTube - https://youtu.be/oif9zVDyq78

Bilibili -  https://www.bilibili.com/video/BV192YXzqE56/


Advanced research on hybrid language models combining **Mamba State Space Models** and **Transformer attention** architectures with extensive multi-GPU training experiments.

## ✨ Key Features

- 🔬 **Extended Research Framework**: 30k step experiments with comprehensive ablation studies
- 🧠 **Hybrid Architecture**: Alternating Mamba SSM and Transformer attention layers
- ⚡ **Multi-GPU Training**: Data parallel training on multiple RTX 4090s
- 📊 **Advanced Monitoring**: Real-time GPU monitoring and comprehensive metrics tracking
- 🎯 **Interactive Inference**: Chat interface for testing trained models
- 📈 **Experiment Analysis**: Automated results analysis and visualization

## 🏗️ Architecture

The hybrid model alternates between:
- **Mamba SSM Layers**: Improved state space models with enhanced numerical stability
- **Transformer Layers**: Multi-head attention with causal masking
- **Flexible Patterns**: Configurable layer arrangements (e.g., "AMAMAMAMAMAMAMAM")

### Model Specifications
- **Hidden Size**: 1024 (2.7x larger than base)
- **Layers**: 16 layers (configurable patterns)
- **Attention Heads**: 16 heads
- **SSM State Size**: 48 states
- **Parameters**: ~1B parameters
- **Sequence Length**: 512 tokens

## 🚀 Quick Start

### 1. Install Dependencies
```bash
pip install -r requirements.txt
```

### 2. Train Extended Model (Multi-GPU)
```bash
# Single experiment
bash run_extended_experiments.sh

# Data parallel training (multiple GPUs)
bash run_data_parallel_experiment.sh
```

### 3. Monitor Training
```bash
# Real-time GPU monitoring
python gpu_monitor.py
```

### 4. Interactive Inference
```bash
# Find available checkpoints
python find_checkpoints.py

# Interactive chat with best model
python interactive_inference.py --checkpoint experiments_extended/amama_16L_data_parallel/checkpoints/best_model.pt

# Single prompt generation
python interactive_inference.py --checkpoint path/to/checkpoint.pt --prompt "The future of AI is" --max_tokens 200
```

## 📁 Project Structure

```
├── experimental_training_extended.py  # Main extended training script
├── train_hybrid_llm.py               # Base model definitions
├── shared_data.py                    # Data management and caching
├── interactive_inference.py          # Interactive inference interface
├── find_checkpoints.py              # Checkpoint discovery utility
├── gpu_monitor.py                    # Real-time GPU monitoring
├── analyze_extended_results.py       # Results analysis and visualization
├── run_extended_experiments.sh       # Extended experiment runner
├── run_data_parallel_experiment.sh   # Multi-GPU experiment runner
├── EXPERIMENT_SETUP_SUMMARY.md       # Detailed experiment documentation
└── requirements.txt                  # Python dependencies
```

## 🔬 Experiments

### Extended Training Configuration
- **Training Steps**: 30,000 (3x longer than base)
- **Documents**: 150,000 (3x more data)
- **Batch Size**: 16 per GPU with gradient accumulation
- **Effective Batch Size**: 64 (16 × 2 GPUs × 2 accumulation)
- **Learning Rate**: 3e-4 with cosine scheduling
- **Warmup**: 3,000 steps

### Data Distribution (Multi-GPU)
- **Data Parallelism**: Each GPU processes unique data subsets
- **Per GPU**: 75,000 documents (with 2 GPUs)
- **Tokens per Step**: 8,192 tokens per GPU
- **Total Training**: ~15,000 steps per GPU (30k total effective)

### Layer Patterns Tested
- `AMAMAMAMAMAMAMAM` (16L): Alternating Attention-Mamba
- Various other patterns with 10-16 layers

## 🎮 Usage Examples

### Interactive Chat Session
```bash
python interactive_inference.py --checkpoint experiments_extended/*/checkpoints/best_model.pt
```

**Interactive Commands:**
- `/temp 0.8` - Set temperature (creativity)
- `/top_p 0.9` - Set nucleus sampling
- `/top_k 50` - Set top-k filtering
- `/tokens 150` - Set max generation length
- `/help` - Show all commands
- `/quit` - Exit session

### Batch Text Generation
```python
from interactive_inference import InteractiveInference

inference = InteractiveInference("path/to/checkpoint.pt")
prompts = ["Once upon a time", "The future of AI", "In a world where"]
results = inference.batch_generate(prompts, max_new_tokens=100)
```

## 📊 Monitoring & Analysis

### Real-time GPU Monitoring
```bash
python gpu_monitor.py
```
- GPU utilization, memory usage, temperature
- Training progress tracking
- Automatic logging to files

### Results Analysis
```bash
python analyze_extended_results.py
```
- Validation loss comparison across patterns
- Training curve visualization
- Performance metrics analysis

## 🔧 Configuration

### Model Configuration (`HybridConfig`)
```python
@dataclass
class HybridConfig:
    hidden_size: int = 1024
    num_layers: int = 16
    num_heads: int = 16
    ssm_state_size: int = 48
    layer_pattern: str = "AMAMAMAMAMAMAMAM"
    max_seq_len: int = 512
    # ... more parameters
```

### Extended Experiment Config (`ExtendedExperimentConfig`)
- Inherits from `HybridConfig`
- Extended training parameters
- Multi-GPU optimization settings
- Advanced monitoring options

## 🚀 Data Management

### Intelligent Caching
- **Tokenized Data Caching**: Preprocessed datasets cached for reuse
- **Distributed Loading**: Automatic data distribution across GPUs
- **Memory Optimization**: Efficient data loading with 4 workers per GPU

### Dataset
- **Source**: HuggingFace SmolLM corpus (cosmopedia-v2)
- **Tokenizer**: SmolLM-135M tokenizer
- **Processing**: 4k character limit per document
- **Split**: 85% train, 15% validation

## 📈 Performance Optimizations

- **Mixed Precision**: Automatic mixed precision with GradScaler
- **Gradient Accumulation**: Effective large batch training
- **Data Parallelism**: Near-linear scaling with multiple GPUs
- **Optimized SSM**: Improved numerical stability and clamping
- **Efficient Attention**: Causal masking with proper padding handling

## 🔍 Key Research Findings

1. **Extended Training**: 30k steps show continued improvement over 10k baseline
2. **Hybrid Benefits**: Alternating patterns outperform pure architectures
3. **Scaling**: Model performs well at 1B parameter scale
4. **Multi-GPU**: Efficient data parallelism with minimal overhead

## 🛠️ Development

### Adding New Experiments
1. Modify `ExtendedExperimentConfig` for new parameters
2. Update layer patterns in experiment scripts
3. Run with `--debug` flag for testing
4. Use `--use_wandb` for experiment tracking

### Custom Inference
- Extend `InteractiveInference` class
- Implement custom generation strategies
- Add new sampling methods

## 📄 License

See [LICENSE](LICENSE) file for details.

## 🤝 Contributing

This is an active research project. Contributions welcome for:
- New hybrid architectures
- Training optimizations
- Analysis tools
- Inference improvements

---

**Note**: This repository contains extended experiments from the original hybrid LLM research, focusing on larger models, longer training, and comprehensive analysis.