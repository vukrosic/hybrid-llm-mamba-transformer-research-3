# ğŸš€ Transformer-Mamba Hybrid LLM Research

LLM combining **Transformer attention** and **Mamba SSM** architectures for efficient language modeling & research.

## âœ¨ Features

- ğŸ§‘â€ğŸ’» **Only 252 Lines of Code**
- ğŸ”„ **Hybrid Architecture**: Alternating Transformer and Mamba layers
- âš¡ **Efficient Training**: Mixed precision, gradient scaling, and optimized data loading
- ğŸ¯ **Flexible Patterns**: Configurable layer arrangements (e.g., "MMAMAMAM")
- ğŸš€ **Multi-GPU Support**: Automatic DataParallel for multi GPUs (only 1 tested)
- ğŸ’¾ **Easy Inference**: Interactive chat mode and text generation

## ï¸ Architecture

The model alternates between:
- **Mamba SSM**: State space model with convolution and simplified parallel processing
- **Transformer**: Multi-head attention with causal masking

##  Quick Start

### 1. Install Dependencies
```bash
pip install -r requirements.txt
```

### 2. Train the Model
```bash
python hybrid_llm.py
```

### 3. Run Inference
```bash
python inference.py --interactive
```
or

```bash
python inference.py --prompt "The future of AI is"
```

## ğŸ“Š Model Configuration

- **Hidden Size**: 384
- **Layers**: 8 (configurable pattern)
- **Sequence Length**: 512
- **Vocabulary**: Auto-detected from tokenizer
- **Parameters**: ~2.5M (configurable)

## ğŸ® Usage Examples

### Interactive Chat
```bash
python inference.py --model model.pt --interactive
```

### Text Generation
```bash
python inference.py --model model.pt --prompt "The future of AI is"
```

## ğŸ“ Project Structure

```
â”œâ”€â”€ hybrid_llm.py    # Training script and model definition
â”œâ”€â”€ inference.py     # Inference and chat interface
â”œâ”€â”€ gpu_monitor.py  # GPU monitoring utilities
â””â”€â”€ requirements.txt # Python dependencies
```

##  Customization

Modify `HybridConfig` in `hybrid_llm.py` to adjust:
- Model dimensions
- Layer patterns
- Training parameters
- Architecture choices

## ğŸ“ˆ Performance

- **Training**: Optimized with AMP, gradient clipping, and efficient data loading
- **Inference**: Fast generation with temperature and top-k sampling
- **Memory**: Efficient attention and SSM implementations

##  Contributing

This is a research project - feel free to experiment with different architectures and configurations!

## ğŸ“„ License

See [LICENSE](LICENSE) file for details.